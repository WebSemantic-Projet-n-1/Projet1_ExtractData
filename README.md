# Web Sémantique - Extract Data

## Project description

This project compares the difficulty of extracting information and knowledge from HTML pages across different eras of the web.

It consists of three search engines that crawl different HTML pages about the 2008–2009 season of the European soccer league, corresponding to the **three parts of the subject**:

* First, **Web 1.0** (1990–2004), also known as the “static web,” mainly composed of static hyperlinks with no user-generated content.

  Extracting information from the DOM is slow, tedious, and repetitive, as described in `engines/web1.py`.

* Then came **[RDF (*Resource Description Framework*)](https://en.wikipedia.org/wiki/Resource_Description_Framework)** and its implementation on the web through RDFa (*RDF in attributes*) with RDF 1.0 in 2008. It provided a means to describe content in HTML pages and initiated the *organization of knowledge on the web*.

  Because RDFa provides structured attributes for information, knowledge extraction becomes much easier. The engine for this part is in `engines/rdfa.py`.

* Finally, in 2012, Google introduced **[Knowledge Graphs](https://blog.google/products-and-platforms/products/search/introducing-knowledge-graph-things-not/)** in their blog post “Things Not Strings.” Knowledge Graphs can be seen as a way to create relationships between RDF attributes.

  The engine for this part is in `engines/knowledge-graph.py` and exploits the RDF graph generated by `build/crawler.py` from the metadata of the “web_3.0” HTML pages.

## Build the project (once)

1. Create Python venv in the project root directory `python -m venv ./.venv`  

2. Activate the venv :
- Windows `.\.venv\Scripts\activate`
- Unix `source .venv/bin/activate`

3. Once in the venv, install the requirements `pip install -r requirements.txt`.

4. Download the dataset [on Kaggle](https://www.kaggle.com/code/alaasedeeq/european-soccer-database-with-sqlite3/input) and rename the downloaded file to `database.sqlite`, then place it at the project root.
 - cURL commant: `curl -L -o soccer.zip https://www.kaggle.com/api/v1/datasets/download/hugomathien/soccer`
 - unzip the file `unzip soccer.zip`

5. Generate the HTML pages : 
- `python ./build/generate_html_pages.py`
- `python ./build/generate_enriched_html_pages.py` (chose 3 : generate both)

6. Generate the knowledge graph :
`python ./build/crawler.py` 

## Run the project

(This assumes the project has been built as per the instructions above)

1. Activate the venv (if not already activated) : 
 - Windows `.\.venv\Scripts\activate`
 - Unix `source .venv/bin/activate`

2. Run the API `fastapi dev main.py` 

3. Go to http://127.0.0.1:8000/


## Project Structure

```
|   database.sqlite             # DB used for generate_html_pages.py
|   knowledge_graph.ttl         # RDF graph generated by crawler.py
|   main.py                     # App entry point
|   README.md
|   requirements.txt
|
+---api
|       api_rdfa.py              # API for the RDFa engine
|       api_web_1.py             # API for the Web 1.0 engine
|
+---build
|       generate_enriched_html_pages.py  # Creates the "web_3.0_*" folders
|       generate_html_pages.py           # Creates the "web_1.0" folder
|       crawler.py                       # Creates the knowledge_graph from enriched pages metadata
|
+---engines                     # Search engines (parts 1, 2, 3)
|       engine_utils.py          # Shared utilities
|       web1.py                  # Search from "web_1.0" HTML pages (part 1)
|       rdfa.py                  # Search from "web_3.0" RDFa pages (part 2)
|       knowledge-graph.py       # Exploits the RDF graph (part 3)
|
+---pages                       # HTML pages used for testing the search engine
|       index.html
|       searchEngine_web-1.0.html
|       searchEngine_web-3.0.html
|
+---web_1.0_output
|       [...]
|
+---web_3.0_jsonld_output
|       [...]
|
+---web_3.0_rdfa_output
|       [...]
```
